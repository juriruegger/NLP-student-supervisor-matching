firstName,lastName,proposal
Anna,Rogers,"Language model analysis: identifying the types of knowledge acquired from language model pre-training."
Anna,Rogers,"Language processing strategies: do NLP models perform well for the right reasons? What strategies should they follow when solving reasoning tasks?"
Anna,Rogers,"Robustness and generalization: do NLP models reliably perform their tasks out of training distribution, and what can we do to help them?"
Anna,Rogers,"NLP system auditing and documentation: establishing the cases where a system is safe to deploy"
Anna,Rogers,"Sustainable NLP: how can we build systems that work well, but don’t require billions of parameters and terabytes of data?"
Alessandro,Bruni,"Training safe robot controllers. Controller safety is an important problem where the controller of a robot should not damage itself or the environment in which it operates. To this purpose, some simulators have been developed like Safety Gym from OpenAI, BRAX by Google, or Mujoco by Deepmind. Differentiable logics allow to have a logical specification of the safety property, and to translate such logic formula into a loss function that can be used in training a neural network. Vehicle is a tool that implements these differentiable logics, and integrates into standard toolkits for training neural networks. The project aim to explore how differentiable logics can help in producing safe controllers."
Alessandro,Bruni,"Formalizing applications of probability theory in the Coq Proof Assistant. Probability theory has many application in computer science: machine learning, statistical algorithms, post-quantum and modern cryptography to name a few. We have been developing two libraries for probability theory with many interesting applications: MathComp-analysis, which is a library for real analysis that includes our work on probability, and Infotheo, which was originally developed to study information theory, but contains now more applications of probabilities. I have many interesting projects in the intersection of machine learning, statistics and probability theory using these libraries. Get in touch if you want to work on such topics."
Alessandro,Bruni,"Analyze a security protocol. Security protocols are used in all aspects of our lives. TLS for example empowers secure communication over the internet, but is not the only protocol. Many protocols include flaws that go undetected even after years that they have been in use. Many of these flaws are logical errors, that is security problems that designers did not think were possible when designing the protocol. These can be detected using tools (like ProVerif) that allow you to model protocols and automatically check that certain security properties hold, or show an attack otherwise. A successful thesis in this area would consider an interesting security protocol, and then conduct a formal analysis using these automated tools. Very often these theses lead to finding mistakes in the specifications or implementation. For example we have helped improve the EDHOC standard for secure IoT communication proposed by Ericsson, and showed some flaws in CAN-bus, a network for controlling electronic components in automobiles. Options for the analysis could be: the Apple keychain/Firefox password storage; a messaging protocol (WhatsApp/Telegram etc.); an Internet protocol; an IoT protocol."
Alessandro,Bruni,"Work on analysis/compilation of secure protocols. Often tools for analyzing security protocols come with limitations: certain properties cannot automatically be checked and require manual abstractions to complete the proofs, and there are certain equations that require special care. One particular problem is when analyzing stateful protocols: protocols that maintain state depending upon which certain actions are enabled or not. Another problem is how to construct secure software once a protocol model has been proven correct: one option is to derive working code from the protocol model that is secure by construction.If you like to work with programming languages, software analysis, logics, compilers, and think that security problems are very relevant in the software landscape, we can find a thesis for you!"
Bernardo Machado,David,"Problem: The majority of current cryptocurrencies and smart contract systems do not offer provable privacy guarantees (e.g. anonymity), allowing any third party to find out who people transfer money to. Most of the very few existing privacy preserving solutions aim at full anonymity, protecting both the identities of users and data about their actions in the system (e.g. the amount of money in anonymous transfers). Such strong privacy guarantees come at a high price in terms of computational/communication complexities and are also incompatible with financial laws and regulations, that dictate that a court of law and competent authorities should be able to compel a citizen to reveal their financial history Possible Solutions: In order to address the situation above it is necessary to propose formal security definitions for cryptocurrency and smart contractsystems with fine-grained privacy guarantees, along with matching constructions. It is also important to design auditing mechanisms for such systems in order to make them compliant with standard KYC/AML financial regulations. Apart from investigating solutions based on standard blockchain systems, we are also interested in exploring state channels for more efficient protocols. Required Experience: All of the suggested projects in this topic involve both theoretical work, that should be suitable for master level students with knowledge of modern cryptgoraphy, and practical implementation work, that should be suitable to students at all levels with reasonable software development experience. We can discuss how to better work on each project according your interests."
Bernardo Machado,David,"Problem: Blockchain consensus protocols are the backbone of modern cryprtocurrency and smart contract systems, being responsible for constructing and updating the blockchain itself while ensuring its immutability. Current protocols suffer from serious efficiency issues, which are specially present in Proof-of-Work based consensus (e.g. Bitcoin's protocol). In practice, the throughput of such systems is limited to only a few tens of transactions per second, meaning that current cryptocurrencies can only process a tiny fraction of the transactions sent to traditional financial systems (around tens of thousands). Such limitations hinders wide scale deployment and adoption of blockchain systems for real world, high throughput applications. Possible Solutions: As in many cases in computer science, a possible solution to this efficiency problem lies in executing multiple copies of the blockchain protocol in parallel. These so called 'blockchain sharding' solutions result in parallel blockchains each being maintained by a separate group of users but still ensuring consistency among each other. A similar solution is to execute particularly heavy smart contracts and application in a 'sidechain', a daugther blockchain derived from a mother chain and used to register transactions for an specific application while ensuring consistency with the mother chain. In both cases, writing meaningful security definitions and matching protocols that achieve sharding or sidechains is a significant challenge. Another option is to look for alternatives to Proof-of-Work (e.g. Proof-of-Stake) and traditional blockchain consesus that can achiebe better performance even with a single instance. Required Experience: The suggested projects in this topic involve mostly theoretical work, which should be suitable for master level students with knowledge of modern cryptgoraphy. We might be able to work on practical evaluation of such protocols together with students who are comfortable with advanced distributed systems programming."
Bernardo Machado,David,"Problem: Most blockchain consensus protocols and blockchain based applications have been designed in an ad-hoc way, i.e. without provable security guarantees. In practice, this has lead to many vulnerabilities in these systems due to bad protocol design decisions (apart from regular software bugs). With these systems handling large amounts of funds, these vulnerabilities lead to almost instant financial losses when exploited. Possible Solutions: In many cases it is not possible to formally prove the security of ad-hoc protocols, but companies and users usually require concrete vulnerabilities to be pointed out in order to consider updating their protocols. As the vast majority of blockchain applications are open source, this provides a good oprtunity to apply practical vulnerability analysis techniques and find such concrete vulnerabilities. Once vulnerabilities are foud, the goal is to propose a protocol level fix and perform responsible disclosure towards the development community. Required Experience: The projects within this topic can be carried out with students of all levels who have a solid programming background, as it will mostly involve code analysis and testing."
Bernardo Machado,David,"Problem: Multiparty Computation (MPC) allows mutuallty distrustful parties to compute a program over their private data while disclosing nothing about this data but the output of the program. While many MPC protocols have been constructed, it is important to improve the efficiency of existing protocols both for general purpose computation (of boolean and arithmetic circuits) and for specific applications (e.g. machine learning over private data). Efficiency can be improved in terms of network communication (e.g. number of protocol rounds and total communication) and of local computational complexity (e.g. local time spent by each party to compute their messages). Possible Solutions: Towards constructing more efficient general purpose MPC protocols, it is important to improve round complexity, since round trip times over the internet make it infeasible to run protocols with many rounds. Moreover, it is important to improve the integration of protocols for computing arithmetic circuits with those for boolean circuits, as each type of circuit is better suited for different kinds of algorithms. More generally, it is interesting to explore the power of preprocessing, where the parties run the bulk of the computation before their actual inputs are known. Besides investigating better protocols for the general case, it is also interesting to explore protocols for specific applications (e.g. machine learning) that might be interesting for end-user products and industry solutions. Required Experience: The projects within this topic involve both theoretical work, that should be suitable for master level students with knowledge of modern cryptgoraphy, and practical implementation work, that should be suitable to students at all levels with reasonable software development experience. We can discuss how to better work on each project according your interests."
Bernardo Machado,David,"Problem: One of the main goals of theoretical cryptography is to understand the minimal building blocks needed for constructing more complicated cryptographic protocols, as well as the efficiency that those can achieve. Two of the main building blocks in cryptographic protocols are Commitments and Oblivious Transfer, which are central to constructing protocols for powerful tasks such as Multiparty Computation and Zero-Knowledge Proofs. Hence, it is important to understand under which assumptions protocols for Commitments and Oblivious Transfer can be constructed, as well as the efficiency limits of such constructions. Possible Solutions: Towards achieving a better understanding of these primitives and their relation to cryptogrtaphic protocols in general, it is interesting to investigate constructions of them under differernt assumptions and the fundamental limits to their efficiency. Towards improving the concrete efficiency of cryptographic protocols, it is interesting to construct more efficiency protocols for Commitments and Oblivious Transfer. Required Experience: The projects within this topic involve heavy theoretical work, that should be suitable for master level students with knowledge of modern cryptgoraphy."
Christian,Hardmeier,"Speech processing and sensitive data. Transcription and anonymisation of sensitive conversations for LLM training. This project is in collaboration with the VIRTU research group at Region Hovedstadens Psykiatri. The goal is to create a dataset of therapist-patient interactions for training large language models, which will be used to analyse and support psychotherapy. Available data consists of a large corpus of audio recordings of psychotherapy sessions. The main challenges of this project will be to adapt automatic speech recogition models to work for optimal results on this data and to anonymise the transcriptions by automatically recognising and editing out personally identifiable information."
Christian,Hardmeier,"Uncertainty quantification and communication. Sensitivity of uncertainty measurements to corpus information. The idea of this project is to study how sensitive the uncertainty metrics we can derive from language model output are to variations in the degree of certainty expressed by the finetuning data. This involves creating finetuning datasets with a controlled level of confidence, using a meta-analysis dataset as input and relating the measured confidence to the confidence in the finetuning dataset. Communicating uncertainty effectively. This project is about aligning the uncertainty a large language model expresses in words with the uncertainty measured numerically with machine learning methods. We want to train a large language model so that the expressed uncertainty, as understood by the user of the language model, matches the uncertainty the model itself 'experiences'. Uncertainty of meaning vs. uncertainty of form. When we measure the uncertainty of a large language model at a token level, this metric conflates the uncertainty of the LLM about the facts it expresses with its uncertainty about how to phrase things. Which of the two is more interesting, depends on the use case (say, grammar checking vs. question answering). Metrics have been proposed to separate the two aspects (e.g. semantic entropy, Kuhn et al., ICLR 2022). This project serves to explore how well the two aspects can be separated and how we can obtain reliable estimates of one type of uncertainty over the other."
Christian,Hardmeier,"Toxicity and bias. Explainable modelling of specific types of toxic speech. Select a specific type of toxic speech and analyse its characteristic properties, then use these insights to create models to recognise this particular type of speech and automatically identify the precise factors making it toxic. I have previously supervised similar student projects on dehumanising language and on threats, and these could potentially be built upon, or you could study an entirely different type of speech. Target-specific modelling of toxic language. A lot of toxic language targets specific groups of victims, e.g., specific demographic groups. The idea of this project is to select a specific group of people that is commonly targeted by toxic behaviour and develop methods for the automatic identification of toxic language targeting that group, recognising and labelling the ways in which toxicity is group-specific."
Christian,Hardmeier,"Referring expressions. Modelling ambiguity in referring expressions. Referring expressions are frequently ambiguous. For instance, in the passage “The bomb exploded violently. It created a huge crater.”, the pronoun “It” could refer either to the bomb itself or to the fact that it exploded violently. The idea of this project is to model and evaluate coreference resolution in a way that preserves and respects this ambiguity and doesn’t force an artificial decision on the process. Cross-lingual realisation of referring expressions. Different languages have different preferences for how to select referring expressions in language generation. This becomes evident when you compare translations of the same text across languages. In this project, you develop automatic methods, based on neural machine translation or large language models, to match up translations of referring expressions across languages even when they are not literal."
Dan Witzner,Hansen,"The proposed DSL will be designed with a syntax structure similar to LaTeX, providing the user with an easy-touse language that is less complex yet still powerful enough to define macros and encapsulate content effectively within Jupyter Notebooks. The primary goals are as follows: Define a lightweight and intuitive syntax, drawing on established language frameworks like Lark for Python, to parse and interpret the DSL. Allow for macro creation and other generalisations to support user-defined elements and reusable components. Ensure compatibility with the JupyterBook environment, allowing users to embed DSL-based content directly within a notebook, maintaining flexibility in Python and Markdown cells. This DSL will facilitate content sharing and definition for platforms like iml.itu.dk, allowing for a smoother integration of custom structures directly within Jupyter-based projects."
Martin,Aumüller,"Privacy-preserving Similarity Search. Avoiding leaks of user information is crucial for the reputation of social networks and other large-scale systems. Hence, all aspects of data processing and managing should be carefully analyzed with regard to possible leaks. The thesis aims at studying privacy issues and privacy-preserving techniques for similarity search systems, a key primitive in several applications (e.g., recommender systems, machine learning, information retrieval) that often handle sensitive data like user preferences or medical records. Work can be carried out in different directions: Attack scenarios on similarity search tools. Describe attack scenarios on similarity search systems and evaluate their impact. This would entail studying different paradigms for similarity search such as locality-sensitive hashing, tree-based techniques, and similarity graphs within their application areas (e.g., recommender systems or deep learning). Privacy-preserving similarity search. Investigate techniques for privacy-preserving similarity search and study their security guarantees. The goal is to build a similarity-search library that offers provable security guarantees. Empirical comparison of privacy-preserving similarity search. Compare two existing methods in terms of their privacy guarantees and running time. For example, (Pagh, Stausholm, 2020) can be used to solve the problem discussed in (Aumüller et al., 2020). There were already two Master theses (see (Aumüller et al., 2020) for an idea) that worked towards building a privacy-preserving similarity search tool. Extending such a tool could be an option. A Bachelor thesis/Research project could be a survey of existing technologies."
Michele,Coscia,"Cultural Data Analysis and Networks: build networks about the production of culture. Examples are artist-band connections using Discogs data, movie networks from IMDb, citations networks from paintings or books, character networks from books and/or graphic novels, notable people networks from Wikipedia, … The idea is to have networks with rich node metadata to study some aspect of cultural production. Examples are new genre classifications, temporal analysis of eras of cultural production, geographical analysis, and more."
Michele,Coscia,"Estimation of Multipolar Polarization: I have developed a measure to estimate how polarized the political discourse is on Facebook and Twitter. However, the measure can only be calculated for two opposing opinions. How can we deal with scenarios where users have multiple opinions, such as supporting multiple different political parties? This project can benefit from NLP techniques to generate word embeddings (GPT, W2V, …) and a machine learning side (PCA, NNMF, t-SNE, …) so not only for network enthusiasts!"
Michele,Coscia,"Build Urban Networks from OpenStreet Data: by using rich data about points of interests, we can characterize cities by how diverse their amenities are, potentially leading to innovative livability scores."
Michele,Coscia,"Human Mobility and Development: Using a new dataset estimating the development level of places in developing countries, we can estimate the link between wealth and the place where one lives. Then we can investigate the effect of lockdowns on the livelihood of people: if human mobility gets much harder, are all people equally affected or is there a disproportionate negative impact for people living in low-income communities?"
Michele,Coscia,"Network Analysis Library using Torch: Torch geometric is a nice library implementing many graph learning functions for GPU processing. There are some GPU bindings for Networkx, but it would be great to develop from the ground up a more complete library with GPU computing at its core. You can help me making it by implementing and testing a handful of network analysis functions."
Michele,Coscia,"Network Generating Model: I have an algorithm that, given a network, it outputs the connection rules underlying it, by exploring frequent patterns. If we are given a set of rules, can we generate a new network from scratch that looks like the one we obtained the rules from?"
Patrick,Bahr,"Implement a functional reactive programming language as an embedded language of an existing language, e.g. Haskell, F#, Scala. This requires some tricks to hijack the type system of the host language (Haskell/F#/Scala/…) to adequately represent the type system of the reactive programming language. The benefit is that programs in your new language can make use of the host language’s features and libraries."
Patrick,Bahr,"Implement a compiler for functional reactive programming language (including a parser, a type checker, and code generator). This gives you maximal freedom to implement the language and do clever things with it (such as type inference, optimisations, more advanced type system features, more efficient memory management). A smaller project could also just focus on one particular aspect of the compiler, e.g. type inference."
Patrick,Bahr,"Use an existing functional reactive programming language to implement a non-trivial system, e.g. a GUI framework, a game, a library for generating smooth animations and visualisations. The aim of this project would be to evaluate how the non-standard type systems of these programming languages influence the development of such software."
Patrick,Bahr,"Property-based testing:Property-based testing is an approach to testing where the programmer first writes properties that their program must satisfy (e.g. sorting a list twice produces the same result as sorting it once) and then uses a tool or library that tests these properties by generating random inputs. The goal of this project is to design and implement such a tool/library for reactive programming. Testing reactive programs is challenging, since they are highly interactive. A main challenge is designing a good language to express the properties that the programmer can test their programs against."
Patrick,Bahr,"Correct-by-construction compilers. Compilers are complex pieces of software with subtle optimisations that are hard to get right. Getting it right is crucial since bugs will propagate to any software that has been built using a faulty compiler. However, compiler bugs are very difficult to spot with standard testing alone. Alternatively, we might hope to formally verify that a compiler implementation works correctly according to its specification. One such approach is to derive the compiler directly from its specification: From a formal specification, the implementation of the compiler is derived by a series of calculation steps – similar to algebraic simplifications of arithmetic expressions. To avoid user errors in this process we use tools such as Coq or Agda that check for mistakes and automate easy steps of the calculation. However, these are generalist tools that make this process laborious and difficult to follow. The goal of this project is to implement a tool that is able to check and partially automate the derivation of compilers from specifications."
Peter,Sestoft,"Type system for sheet-defined functions in spreadsheets. The goal of this project is to define and implement a type system for sheet-defined functions as implemented in Funcalc. Funcalc is described in the book Sestoft: Spreadsheet Implementation Technology, MIT Press 2014. The purpose is to improve both safety and performance of sheet-defined functions, by avoiding the overhead of boxing values at function calls. Some promising work in this direction was done by Poul Broennum in a previous project."
Peter,Sestoft,"Computations on GPGPUs (graphics cards) Modern graphics cards are ultrafast computers, also called GPGPUs (general-purpose graphics processing units). They can perform certains computations 10 to 300 times faster than a typical CPU. Programming them requires special techniques and libraries (Nvidia CUDA, OpenCL, or similar). In this project, you must implement some numeric computations, typically linear algebra, measure their performance, and tune them for optimal speed."
Peter,Sestoft,"Generation of specialized collection libraries The C5 generic collection library is a comprehensive library for C#/.Net 2.0 that provides very rich functionality such as sublist views, update events, hash-indexed lists, persistent trees, cached hashcodes and much more. One drawback of this rich functionality is that user programs pay (in terms of memory requirements) also for the advanced features that they do not use. The purpose of this project is to investigate the automatic generation/selection of only the relevant components (classes, methods and fields) for any particular use of the library, to minimize memory consumption and make the library as fast as possible."
Peter,Sestoft,"Applications of run-time code generation in Java or C#. The Java or JVM platform from Sun Microsystems/Oracle and the C# or Common Language Runtime/.Net platform from Microsoft provide good support for generation and fast execution of bytecode by a running program. This opens new implementation possibilities for serialization/deserialization, communication protocols, encryption, image transformation and image analysis algorithms."
Rob,van der Goot,"How to learn from different sourcesPermalink. When training an NLP model on a variety of datasets, from different sources (and with different underlying distributions), it could be informative for the model to know about the origin of the text. A variety of methods can be used to inorporate this information, including: Dataset embeddings: Many Languages, One Parser. Just include the domain as a special start token, like [SOCIAL] or [NEWS]. Let the model predict the origin of the input as auxiliary task. These methods have shown promising results in isolated setups, but have never been consistently compared."
Rob,van der Goot,"Cross-domain lexical normalizationPermalink. Lexical normalization is the task of converting non-standard language to standard language on the word level. For this task, most of the focus has been on data from Twitter. However, there are many other types of non-standard language. This project can ask two questions: 1) how does performance drop on other domains 2) how can we build more robust lexical normalization models. Most existing datasets can be found on: MultiLexNorm: A Shared Task on Multilingual Lexical Normalization. MultiLexNorm 2. This project includes data collection and annotation, but it should be noted that annotation for this task is relatively fast"
Rob,van der Goot,"Using agents for cultural-aware language adaptations of children televisionPermalink. Cartoons can be a great resource for learning a language. However, they are often only available in a few languages. Adapting these to another language is expensive, as it requires understanding of the context, cultural aware translation (much of the language use is conversational, unlike most translation datasets), and text to speech. For each of these steps, a specific NLP model (i.e. agent) can be used. One main challenge is the evaluation, where human judgements are probably necessary."
Rob,van der Goot,"seq2seq versus sequence classification modelsPermalink. Recently, sequence-to-sequence (seq2seq) have become more and more powerful. A recent paper has shown that seq2seq models can perform on-par with sequence classification models through conversion of tasks to sequence generation tasks. However, the comparison is not direct, so it would be interesting to make a more direct comparison of an auto-encoder language model versus a generative one. Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing"
Rob,van der Goot,"culture2vecPermalink. NLP models are known to be bad at understanding cultural dimensions. There are many open directions to improve this; one of them would be predicting cultural dimensions (for example from Hofstede, see reference below) based on texts, another one would be incorporating this information during training and prediction. Hofstede’s cultural dimensions"
Rob,van der Goot,"Effect of temperature on sampling of language modelsPermalink. During text generation with language model, one of the desiderata is to make the text human-like. Temperature is a smoothing technique which controls the randomness of language models outputs. This is one of the main parameters that controls the “style” of the output. This project investigates how temperature relates to human-ness of generated text, as judged by (a) human(s). How to generate text: using different decoding methods for language generation with Transformers"
Rob,van der Goot,"Early stopping strategiesPermalink. When training a neural model (in NLP), it is common to use a development split to decide when to stop training. However, this has some downsides (see: We Need to Talk About train-dev-test Splits ). There have been some alternative strategies proposed for model selection/early stopping. One strategy is to train for a specific pre-defined number of steps, another one is to look at train loss. However, to the best of my knowledge, no comparison of these approaches has been done. Active Learning Helps Pretrained Models Learn the Intended Task. Model Selection for Cross-Lingual Transfer"
Rob,van der Goot,"Effect of sociodemographic factors on language usePermalink. Recent work has shown that including the origin of a text instance can improve performance on NLP tasks. However, it is unclear which specific sociodemographic attributes correlate with language use. Recent efforts on annotation of social media data could give us more insights. Women’s Syntactic Resilience and Men’s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing. Gender Differences in English Syntax. Cross-lingual syntactic variation over age and gender"
Rob,van der Goot,"Translation, generation, or manual labour for instruction tuningPermalink. Instruction tuning refers to the phase of language model training where the model learns how to respond to tasks. Many instruction tuning datasets have been created for English recently. However, for other languages there is usually (almost) no manually created data. In this case, people usually use translated instructions from English data, or instructions generated by larger, more accurate language models. However, a systematic comparison is lacking. This project will investigate the amounts of data and costs of creating data with the different approaches."
Rob,van der Goot,"Multi-lingual lexical normalizationPermalink. Lexical normalization is the task of converting social media language to its canonical equivalent. Most literature on this problem is only tackling this task for one language. But in 2021, MultiLexNorm was introduced; including normalization datasets for 13 language variants. A wide variety of models was evaluated on this new benchmark, however all of these trained a single model for each language. However, at least three of these models can be used in a multi-lingual or cross-lingual setup, which could enable more efficiency, performance and transfer to new language for which no annotated data is available. MultiLexNorm: A Shared Task on Multilingual Lexical Normalization. Website for dataset: Multi-LexNorm"
Rob,van der Goot,"Tokenization of social media dataPermalink. In many NLP benchmarks, tokenized texts are assumed as input to our models. For standard domains, tokenization can be considered a solved problem, however, for social media text tokenization is non-trivial. The goal of this project is to create a multi-lingual corpus and model for this task. Steps include: Finding the original utterances of Multi-LexNorm. Create a gold standard dataset based on the original and the tokenized data. Evaluate existing tokenizers and train your own. Some related work: Universal Word Segmentation: Implementation and Interpretation twokenize nltk.TweetTokenizer"
Rob,van der Goot,"Efficient language identification for many languagesPermalink. Language identification is a standard NLP task, which is often considered to be solved. However, most current classifiers only support around 100 languages, or are not publicly available. This project makes use of the LTI LangID Corpus(with >1300 languages), and asks the question: how can we efficiently handle such a large label space, and such a wide variety in input-features. Relevant previous work: Non-linear Mapping for Improved Identification of 1300+ Languages A Fast, Compact, Accurate Model for Language Identification of Codemixed Text Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"
Rob,van der Goot,"Cross-domain language identificationPermalink. Most language identification models are trained and evaluated on a single domain. A cross-domain dataset can however relatively easily be compiled, and allows for testing existing language identification models for robustness. Some existing resources for a variety of domains are listed below: Accurate Language Identification of Twitter Messages MassiveSumm: a very large-scale, very multilingual, news summarisation dataset LTI LangID Corpus Fandom Wiki’s"
Rob,van der Goot,"Dependency parsing of Danish social media dataPermalink. Dependency parsing is the task of finding the syntactic relations between words in a sentence. Chapter 14 of the Speech and Language Processing book contains a nice introduction to this topic. Many different languages and domains have been covered by the recent Universal Dependencies project. However, for language types not covered performance is generally lower. Recently, we have collected some non-canonical data samples: DaN+, for which it is uncertain how well current methods would perform. The goal of this project would be to annotate a small sample of Danish social media data to evaluate parsers. Then, a variety of approaches of adapting the parser could be studied, including the ones mentioned below: Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning: A Faroese Case Study A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages How to Parse Low-Resource Languages: Cross-Lingual Parsing, Target Language Annotation, or Both? Modeling Input Uncertainty in Neural Network Dependency Parsing"
Rob,van der Goot,"Strategies for Morphological TaggingPermalink. Morphological tagging is the task of assigning labels to a sequence of tokens that describe them morphologically. This means that one word can have 0-n labels. There has been a variety of architectures proposed to solve this task, however it is unclear which method works best in which situation. In this project you can make use of the Universal Dependencies data, which has annotation for morphological tags for many languages. You can use the MaChAmp toolkit, or implement a BiLSTM tagger yourself, and evaluate at least the three most common strategies: Predict the concatenation of the tags as one label (same as POS tagging, but with more labels) Predict morphological tags as a sequence (like machine translation) View the task as a multilabel prediction problem (Get a probability for each label, and set a cutoff threshold) Related reading: The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis."
Rodrigo Moreno,Garcia,"Large-scale co-evolution in a Massively Multi-agent Game environment. Use the Neural MMO environment (https://openai.com/research/neural-mmo) to co-evolve populations of thousands/millions of neural network-controlled agents. Under what circumstances would interesting group dynamics evolve? What environmental factors drive agents to form different species or when would they start to collaborate?"
Rune Møller,Jensen,"Vessel Stowage Optimization. The purpose of this project is to develop new optimization algorithms for container vessel stowage planning. This problem is too large to solve optimally. Thus, the goal of the project is to develop fast and accurate approximation algorithms. A wide range of techniques may be applied to this end including concrete heuristics, large neighborhood search, symbolic representations, integer programming and hierarchical decompositions."