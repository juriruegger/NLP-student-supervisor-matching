firstName,lastName,proposal
Anna,Rogers,"I’m really interested in diving into how language models learn and what kind of knowledge they pick up during their pre-training phase. It’s fascinating to think about how these models can understand and generate human-like text, but I want to dig deeper into the specifics of what they actually know.

I plan to explore the different types of knowledge that these models acquire, like facts about the world, grammar rules, and even cultural nuances. I think it would be cool to analyze how well they can handle various tasks based on this knowledge. For instance, can they answer questions accurately or generate coherent stories? 

One challenge I anticipate is figuring out the best ways to measure and categorize this knowledge. There’s a lot of complexity involved, and I’m not sure how straightforward it will be to pinpoint what the models know versus what they can just mimic. 

Overall, I’m excited about this project because it could shed light on the inner workings of language models and help us understand their strengths and limitations better. Plus, it might have implications for how we use these models in real-world applications."
Anna,Rogers,"I’m really interested in diving into how language processing works, especially when it comes to natural language processing (NLP) models. I want to explore whether these models are actually getting things right for the right reasons. It’s fascinating to think about how they tackle reasoning tasks and what strategies they use to come up with their answers.

One of the things I find intriguing is the possibility that these models might be relying on patterns or shortcuts rather than truly understanding the language. I want to dig deeper into this and figure out what’s going on under the hood. Are they just mimicking what they’ve seen in the data, or do they have a genuine grasp of the concepts?

I’m also curious about the different approaches these models might take when faced with reasoning challenges. It would be great to identify which strategies lead to better performance and why. This could help us understand not just how these models work, but also how we can improve them.

Overall, I think this project could shed light on some important questions about the capabilities and limitations of NLP models. Plus, it’s a topic that really excites me, and I’m eager to see what I can uncover!"
Anna,Rogers,"I’m really interested in exploring how well NLP models hold up when they’re faced with data that’s different from what they were trained on. It seems like a big question in the field: can these models actually do their jobs when they encounter new situations? I want to dig into whether they can generalize their skills beyond the training data and what strategies we might use to improve their performance in these scenarios.

I think this topic is super relevant, especially since we rely on these models for so many applications. It’s kind of concerning to think that they might struggle when faced with real-world data that doesn’t match their training sets. I’m curious about the challenges that come with this, like figuring out what kinds of data they struggle with and why. 

I’d love to look into different techniques that could help boost their robustness. Maybe we could explore things like data augmentation or transfer learning to see if they can adapt better to new situations. Overall, I’m excited about the potential to make these models more reliable and effective, and I think this project could really contribute to that goal."
Anna,Rogers,"I’m really interested in diving into the world of NLP systems, especially when it comes to figuring out how to audit and document them effectively. With so many AI tools popping up, it’s crucial to understand when it’s actually safe to roll them out. 

I want to explore the different scenarios where these systems can be trusted. There’s a lot of uncertainty around this, and I think it’s important to identify the specific conditions that make a system reliable. I’m hoping to look into various factors that contribute to a system’s safety, like how it handles data, its performance in real-world situations, and any potential biases it might have.

This project excites me because I believe that ensuring the safety of NLP systems is vital for their acceptance and use in society. I know there will be challenges, like figuring out the best ways to evaluate these systems and documenting everything clearly. But I’m eager to tackle these issues and contribute to making NLP technology more trustworthy."
Anna,Rogers,"I’m really interested in exploring how we can create natural language processing systems that are effective without needing massive amounts of data or tons of parameters. It seems like a big challenge, especially with the trend of developing super complex models that consume a lot of resources. I want to dig into ways we can make these systems more efficient and accessible. 

I think it’s crucial to find a balance between performance and sustainability. It’s not just about making things work well; it’s also about being responsible with our resources. I’m curious about techniques like transfer learning or using smaller, more focused datasets that could help us achieve good results without the heavy lifting. 

There’s a lot of uncertainty in this area, and I’m excited to tackle those challenges. I believe that by focusing on sustainability in NLP, we can open up new possibilities for innovation while being mindful of our environmental impact. This project could really contribute to a more responsible approach in tech, and I’m eager to see what I can discover."
Allesandro,Bruni,"I'm really interested in working on a project about making robot controllers safer. It's super important that these controllers don’t end up harming themselves or messing up their surroundings. There are some cool simulators out there, like Safety Gym from OpenAI, BRAX from Google, and Mujoco from DeepMind, that help with this. 

I’ve learned that differentiable logics can help us define safety rules clearly and turn those rules into loss functions for training neural networks. There's this tool called Vehicle that uses these logics and fits into the usual frameworks for training neural networks. 

For my project, I want to dive into how these differentiable logics can actually help us create safer robot controllers. I think it’ll be a challenge, but I’m excited to see what I can discover and how it can make a difference in robotics!"
Allesandro,Bruni,"I’m really excited about diving into the world of probability theory and its applications, especially in computer science. There’s so much potential in areas like machine learning, statistical algorithms, and even cryptography. I’ve been working with two cool libraries that focus on probability: MathComp-analysis, which is all about real analysis and includes some of my work on probability, and Infotheo, which started out focused on information theory but has expanded to cover a lot of probability applications.

I’ve got a bunch of project ideas that sit at the crossroads of machine learning, statistics, and probability using these libraries. I think there’s a lot to explore, and I’d love to collaborate with anyone interested in these topics. If you’re keen to tackle some of these challenges together, let’s connect!"
Allesandro,Bruni,"I want to dive into analyzing a security protocol because they play such a huge role in our daily lives. Take TLS, for instance; it’s crucial for keeping our online chats and transactions safe, but it’s just one of many protocols out there. What’s interesting is that even after being around for years, some of these protocols still have hidden flaws that nobody noticed. A lot of these issues come from logical errors—basically, things the designers didn’t think could happen when they created the protocol.

I’m really keen on using tools like ProVerif to model these protocols and see if they actually hold up under scrutiny. It’s pretty cool that these tools can automatically check for security properties or reveal potential attacks. I think a solid project in this area would involve picking a security protocol that stands out and then doing a thorough analysis with these automated tools. 

I’ve heard that many students who’ve tackled similar projects ended up uncovering mistakes in the protocol specs or how they were implemented. For example, some work has helped refine the EDHOC standard for secure communication in IoT devices, and there were also findings related to CAN-bus, which is used in cars to control electronic parts. 

I’m considering a few options for my analysis, like the Apple keychain or Firefox’s password storage, or maybe even a messaging protocol like WhatsApp or Telegram. I’m also thinking about looking into an Internet protocol or something related to IoT. I’m excited about the potential challenges and discoveries that could come from this project!"
Allesandro,Bruni,"I’m really interested in diving into the world of secure protocols for my project. I’ve noticed that a lot of the tools out there for analyzing these protocols have some pretty big gaps. For instance, there are certain properties that can’t be checked automatically, which means you often have to do a lot of manual work to get the proofs right. Plus, some equations need extra attention, which can be tricky.

One area that really stands out to me is stateful protocols. These are the ones that keep track of their state based on what actions are allowed or not, and I think they present some unique challenges. Another thing I want to explore is how to actually build secure software once we’ve proven that a protocol model is correct. One approach could be to generate code directly from the protocol model, ensuring it’s secure from the get-go.

I’m excited about this topic because I believe security is super important in today’s software world. I enjoy working with programming languages, software analysis, and compilers, so I think this project could be a great fit for me. I’m looking forward to figuring out the best way to tackle these challenges!"
Bernardo,Machado David,"I've been thinking a lot about the issues with cryptocurrencies and smart contracts, especially when it comes to privacy. Right now, most of them don’t really keep users' identities safe, which means anyone can see who’s sending money to whom. There are a few solutions out there that try to keep everything anonymous, but they often make things super complicated and can clash with financial laws. It’s a tricky balance because while we want privacy, regulations require that authorities can access financial histories when needed.

To tackle this, I want to come up with some solid security definitions for cryptocurrencies and smart contracts that offer better privacy without being overly complex. I think it’s also crucial to create ways to audit these systems so they can still follow KYC and AML rules. I’m really curious about looking into state channels too, as they might help us develop more efficient protocols.

For this project, I believe it’ll involve both some theoretical work, which should be manageable for someone at the master’s level who knows a bit about modern cryptography, and practical coding tasks that anyone with decent software skills can handle. I’m excited to explore how we can shape this project based on what I’m most interested in!"
Bernardo,Machado David,"I want to dive into the world of blockchain and tackle some of the big issues with how consensus protocols work. These protocols are super important for cryptocurrencies and smart contracts because they help build and update the blockchain while keeping everything secure. But here’s the catch: a lot of the current systems, especially those using Proof-of-Work like Bitcoin, are really slow and can only handle a handful of transactions at a time. This is a huge problem since traditional financial systems can manage way more transactions, which makes it tough for blockchain tech to be used in real-world applications that need to process a lot of data quickly.

One idea I’m excited about is blockchain sharding, where you run multiple versions of the blockchain at the same time. Each version would be managed by different groups of users, but they’d still work together to keep everything consistent. Another approach could be using sidechains, which are like mini blockchains that handle specific tasks while still connecting back to the main chain. However, figuring out how to keep everything secure and working smoothly in these setups is a real challenge.

I’m also interested in exploring alternatives to Proof-of-Work, like Proof-of-Stake, which might offer better performance even when running just one instance. I think this project would be a great fit for me since it leans more towards theoretical work, and I’ve got a solid background in modern cryptography. Plus, I’d love to collaborate with others who are into advanced programming for distributed systems to see how we can practically evaluate these protocols. Overall, I’m really looking forward to digging into this topic and figuring out how to make blockchain technology more efficient and usable in the real world."
Bernardo,Machado David,"I've been thinking a lot about blockchain lately, especially how most consensus protocols and applications seem to be thrown together without solid security measures. This kind of haphazard design has led to a bunch of vulnerabilities that can really hurt users, especially since these systems often deal with a lot of money. When these weaknesses are exploited, it can lead to quick financial losses, which is pretty scary.

I believe there’s a way to tackle this issue. While it’s tough to formally prove that these ad-hoc protocols are secure, companies and users usually want specific vulnerabilities highlighted before they’ll even think about making updates. Since most blockchain projects are open source, I see a great chance to dive into practical vulnerability analysis. By identifying real vulnerabilities, I can then suggest fixes at the protocol level and responsibly share my findings with the development community.

I think this project is a perfect fit for me because I have a solid programming background, and I’m excited about the idea of digging into code analysis and testing. I know there will be challenges along the way, but I’m eager to learn and contribute to making these systems safer for everyone."
Bernardo,Machado David,"I’m really interested in diving into the world of Multiparty Computation (MPC). It’s a cool way for people who don’t trust each other to work together on data without revealing anything except the final results. There are already a bunch of MPC protocols out there, but I think there’s a lot of room to make them faster and more efficient, especially when it comes to general computing tasks and specific uses like machine learning with private data.

One of the big issues I see is that many of these protocols take too long because they involve too many rounds of communication. With how slow the internet can be, having lots of rounds just isn’t practical. I’d love to focus on reducing the number of rounds needed. Also, I think it’s important to find a way to combine the protocols for arithmetic and boolean circuits since they work better for different types of algorithms. 

Another area I find fascinating is preprocessing, where parties can do a lot of the heavy lifting before they even know their actual inputs. This could really speed things up. Besides just improving the general protocols, I want to look into how these ideas can be applied to real-world problems, like in machine learning, which could be super useful for businesses and everyday products.

I know this project will involve both theory and practical coding, which is great because I have a solid background in software development and I’m eager to learn more about modern cryptography. I’m excited to discuss how I can contribute to this project based on my interests and skills!"
Bernado,Machado David,"I’m really interested in diving into the world of theoretical cryptography, especially when it comes to figuring out the basic components that make up more complex cryptographic systems. Two key elements that keep popping up are Commitments and Oblivious Transfer. These are super important for building things like Multiparty Computation and Zero-Knowledge Proofs, which are pretty powerful tools in the field. 

I think it’s crucial to explore what assumptions we need to make in order to create effective protocols for these components, as well as to understand how efficient these protocols can actually be. I want to look into different ways to construct Commitments and Oblivious Transfer under various assumptions and see what the limits are on their efficiency. 

I’m also keen on finding ways to make these protocols more efficient in practice. It seems like there’s a lot of room for improvement, and I’d love to work on developing better protocols for both Commitments and Oblivious Transfer. I know this project will involve a lot of theoretical work, but I’m ready for the challenge and I believe my background in modern cryptography will help me tackle it."
Christian,Hardmeier,"I’m really excited about this project focused on speech processing and handling sensitive information. The idea is to work with the VIRTU research group at Region Hovedstadens Psykiatri to put together a dataset of conversations between therapists and patients. This dataset will help train large language models that can analyze and enhance psychotherapy practices.

We have access to a ton of audio recordings from therapy sessions, which is great, but it also comes with its own set of challenges. One of the biggest hurdles will be tweaking automatic speech recognition systems so they can accurately process this specific type of data. Plus, we need to figure out how to anonymize the transcriptions effectively, making sure we remove any personal details that could identify the individuals involved. It’s a bit daunting, but I’m really interested in how we can balance the need for data with privacy concerns."
Christian,Hardmeier,"I'm really interested in diving into how we measure and communicate uncertainty in language models. My project will focus on figuring out how sensitive our uncertainty measurements are to the data we use for fine-tuning. Basically, I want to see how changes in the confidence level of the fine-tuning data affect the uncertainty metrics we get from the model's output. To do this, I plan to create fine-tuning datasets that have a specific level of confidence, using a meta-analysis dataset as a starting point. Then, I’ll look at how the confidence we measure relates to the confidence in the fine-tuning data itself.

Another big part of my project is about making sure that the uncertainty expressed by the language model matches up with the uncertainty we can measure using machine learning techniques. I want to train the model in a way that the uncertainty it communicates aligns with what it actually ""feels."" This is important because if users can’t trust the model’s uncertainty, it can lead to misunderstandings.

I also want to explore the difference between uncertainty in meaning and uncertainty in form. When we look at uncertainty at the token level, it can get tricky because it mixes up the model's uncertainty about the facts with its uncertainty about how to phrase things. Depending on what we’re using the model for—like grammar checking versus answering questions—one type of uncertainty might be more relevant than the other. There are some metrics out there that try to separate these two aspects, like semantic entropy, but I want to dig deeper into how well we can distinguish between them and how we can get reliable estimates for each type. 

Overall, I think this project could really help improve how we understand and communicate uncertainty in language models, which is super important for their practical use."
Christian,Hardmeier,"I want to dive into the world of toxic speech and how it affects different groups of people. My plan is to focus on a specific type of harmful language and really break down what makes it toxic. I think it would be super interesting to analyze its unique traits and then use that info to build models that can spot this kind of speech automatically. 

I've seen some projects before that looked at things like dehumanizing language and threats, so I could either build on those ideas or explore something totally different. I’m particularly interested in how toxic language often targets certain groups, like specific demographics. My goal is to pick one of these groups that frequently faces this kind of negativity and come up with ways to automatically identify the toxic language aimed at them. I want to highlight how the toxicity can vary depending on who it’s directed at, and I think this could really shed light on the issue. 

I know there will be challenges, like figuring out the best methods to analyze and label this language, but I’m excited about the potential impact of this work. It feels important to understand and address how language can harm specific communities, and I’m eager to contribute to that understanding."
Christian,Hardmeier,"I want to dive into how we use referring expressions and the confusion that can come with them. You know how sometimes a word can mean different things? Like in the sentence, “The bomb exploded violently. It created a huge crater,” the word “It” could mean either the bomb or the explosion itself. My goal is to figure out a way to model this kind of ambiguity without forcing a clear-cut answer. 

I’m also really interested in how different languages handle referring expressions. It’s pretty fascinating to see how translations can vary. For example, when you look at the same sentence in different languages, the way they refer to things can change a lot. I want to create some automatic methods using neural machine translation or large language models to help match these expressions across languages, even when they don’t translate directly. This project feels important to me because it tackles real challenges in understanding language and communication."
Dan,Witzner Hansen,"I’m really excited about this project where I want to create a new domain-specific language (DSL) that’s inspired by LaTeX. The idea is to make it super user-friendly and not too complicated, while still being powerful enough to let users create macros and organize their content well in Jupyter Notebooks. 

Here’s what I’m aiming for: First, I want to come up with a simple and clear syntax that’s easy to pick up. I’m thinking of using something like Lark for Python to help with parsing and interpreting the DSL. Next, I want to make sure users can create their own macros and reusable components, which I think will really enhance the experience. 

Another big goal is to make sure this DSL works seamlessly with JupyterBook. This way, users can easily add DSL content right into their notebooks without any hassle, while still being able to use Python and Markdown cells. I believe this will make sharing and defining content on platforms like iml.itu.dk much easier, allowing for a better integration of custom structures in Jupyter projects. I’m looking forward to tackling the challenges that come with this and seeing how it all comes together!"
Martin,Aumüller,"I’m really interested in diving into the topic of privacy-preserving similarity search for my project. With all the concerns about user data leaks, especially on social networks and big systems, it’s super important to figure out how to handle data without compromising privacy. My goal is to explore the privacy challenges and techniques that can help keep user info safe, especially in systems that deal with sensitive data like personal preferences or health records.

There are a few directions I could take with this. One idea is to look into different attack scenarios that could target similarity search tools. I’d want to analyze how these attacks could affect the systems, which means I’d need to study various methods for similarity search, like locality-sensitive hashing and tree-based approaches, and see how they apply in areas like recommendation systems or deep learning.

Another angle I’m considering is investigating existing techniques that focus on privacy-preserving similarity search. I’d like to understand their security features better and maybe even work on creating a library that guarantees some level of security for similarity searches.

I also think it would be valuable to compare two current methods in terms of their privacy protections and how fast they run. For instance, I could look at the work done by Pagh and Stausholm in 2020 and see how it relates to the findings from Aumüller et al. in the same year. There have already been a couple of Master’s theses that touched on building tools for privacy-preserving similarity search, so extending one of those could be a solid option for me. Alternatively, I could do a survey of the existing technologies as a Bachelor’s thesis or research project. Overall, I’m excited about the potential impact of this work and the challenges it presents!"
Michele,Coscia,"I’m really interested in diving into how culture gets produced and shared, and I think a cool way to explore this is by building networks that show connections in cultural data. For instance, I could look at how artists and bands are linked using data from Discogs, or maybe create networks based on movies from IMDb. There’s also the option of analyzing citation networks from various artworks or books, or even mapping out character relationships in novels and graphic novels. 

I find it fascinating to see how different figures are connected through platforms like Wikipedia, and I think there’s a lot to uncover about cultural production through these networks. I’d love to analyze things like how genres evolve over time, how cultural trends shift geographically, and other aspects that could reveal patterns in how culture develops. There’s definitely a lot of potential here, but I know it’ll come with its own set of challenges, especially in gathering and interpreting all that data. Overall, I’m excited about the possibilities this project could open up!"
Michele,Coscia,"I've been working on a way to figure out how polarized political discussions are on platforms like Facebook and Twitter. The catch is that my current method only works for two opposing views. This got me thinking: what about people who have mixed opinions or support several different parties? It feels like a big gap that needs to be addressed. 

I believe this project could really take off by using some cool natural language processing tools to create word embeddings, like GPT or Word2Vec. Plus, we could dive into some machine learning techniques like PCA, NNMF, or t-SNE to analyze the data better. This isn't just for the tech-savvy folks; I think it could really help anyone interested in understanding the complexities of online political conversations. I'm excited about the potential challenges and discoveries that lie ahead!"
Michele,Coscia,"I want to dive into how we can use OpenStreetMap data to explore urban networks. There’s a ton of information out there about different points of interest in cities, and I think it could really help us understand how diverse a city’s amenities are. By analyzing this data, I hope to come up with some fresh ideas for livability scores that reflect what makes a city enjoyable to live in. It’s exciting to think about how this could change the way we view urban spaces, but I know there will be challenges in figuring out the best way to analyze and interpret all this data."
Michele,Coscia,"I’m really interested in exploring how where we live affects our chances of doing well, especially in developing countries. I found this new dataset that gives a pretty good idea of how developed different areas are, and I think it could help us understand the connection between wealth and location. 

I also want to dive into how lockdowns have changed things for people. When movement is restricted, do all communities feel the impact the same way? I’m curious if folks in low-income areas are hit harder than others. It seems like a crucial topic, especially with everything that’s been happening lately, and I want to see if I can uncover some insights that could help address these inequalities."
Michele,Coscia,"I’m really excited about the idea of creating a network analysis library using Torch. I’ve been exploring Torch Geometric, and it’s pretty cool how it handles graph learning on GPUs. While there are some GPU options for Networkx, I think it would be awesome to build a more comprehensive library that focuses on GPU computing right from the start. I’d love to dive into this project by developing and testing a few key network analysis functions. It’ll be a great challenge, and I’m eager to see how I can contribute to something that could really enhance the way we work with graphs!"
Michele,Coscia,"I’ve been thinking about a cool project where I want to dive into network generation. Basically, I’ve got this algorithm that takes a network and figures out the rules that connect everything by looking for patterns that pop up a lot. The big question I want to tackle is whether we can take a set of these rules and create a brand-new network that resembles the original one. It’s kind of like reverse engineering but in a fun way! I’m really curious about how this works and what challenges might come up along the way. It feels like there’s a lot to explore here, and I’m excited to see if we can actually pull it off."
Patrick,Bahr,"I’m really interested in creating a functional reactive programming language that can work within an existing language like Haskell, F#, or Scala. The idea is to find clever ways to adapt the type system of the host language so that it can properly support the type system of this new reactive language. 

What excites me about this project is the potential to leverage the features and libraries of the host language, which could make programming in this new language much more powerful and flexible. However, I know there will be challenges, especially when it comes to navigating the complexities of type systems and ensuring everything integrates smoothly. I’m eager to dive into this and see how I can make it work, even if it means tackling some tricky problems along the way."
Patrick,Bahr,"I’m really excited about the idea of creating a compiler for a functional reactive programming language. My plan is to build the whole thing from scratch, which means I’ll be working on the parser, type checker, and code generator. I think this project will let me explore a lot of creative options, like figuring out type inference and maybe even adding some cool optimizations. 

I’m particularly interested in how to make the type system more advanced and improve memory management, since those are areas that can really impact performance. However, I know this is a big undertaking, so I might also consider narrowing my focus to just one part of the compiler, like diving deep into type inference. 

I’m looking forward to the challenges this project will bring, especially since I’ll be learning a lot about how compilers work and how to make a programming language more efficient. Overall, I think this project will be a great way to combine my interests in programming languages and software development."
Patrick,Bahr,"I’m really interested in diving into functional reactive programming (FRP) and how it can be used to build something cool, like a game or a GUI framework. I want to pick a language that’s already set up for FRP and see how its unique type system affects the way I develop the project. 

I think it’ll be fascinating to explore how these non-traditional type systems can change the way I approach coding. For instance, I’m curious about how they might make it easier or harder to create smooth animations or visualizations. There’s definitely a lot to figure out, and I expect there will be some challenges along the way, especially when it comes to understanding the quirks of the language I choose. 

Overall, I’m excited to see how this project unfolds and what I can learn about both FRP and the specific language I end up using. It feels like a great opportunity to blend creativity with technical skills, and I can’t wait to get started!"
Patrick,Bahr,"I'm really interested in diving into property-based testing for my project. Basically, this method involves writing down certain rules or properties that a program should follow—like if you sort a list twice, you should get the same result as sorting it just once. Then, you use a tool or library to check these properties by throwing random inputs at the program. 

For my project, I want to create a tool or library specifically for reactive programming, which is a bit tricky because these types of programs are super interactive. One of the biggest hurdles I see is figuring out how to create a clear and effective way to express the properties that developers can use to test their programs. I’m excited about the challenge and can’t wait to see how I can make this work!"
Patrick,Bahr,"I want to dive into the world of compilers, which are these really intricate pieces of software that can be tricky to get right. It’s super important to nail down the details because any bugs in a compiler can mess up all the software built on top of it. The problem is, finding these bugs through regular testing is often a real challenge. 

One interesting way to tackle this is by formally verifying that a compiler does what it’s supposed to do based on its specifications. This means we can actually derive the compiler from its specs through a series of logical steps, kind of like simplifying math problems. To help with this, we can use tools like Coq or Agda, which are designed to catch errors and automate some of the easier parts of the process. But honestly, these tools can be pretty overwhelming and make the whole thing feel tedious.

So, for my project, I want to create a new tool that can help check and even automate parts of the process of deriving compilers from their specifications. I think this could really make things easier and more efficient, and I’m excited about the potential impact it could have on the field."
Peter,Sestoft,"I’m really interested in working on a project that focuses on creating a type system for functions defined in spreadsheets, specifically using Funcalc. I came across this concept in a book by Sestoft about spreadsheet technology, and it got me thinking about how we can make these functions safer and faster. 

Right now, one of the main issues is that when you call these functions, there’s a lot of unnecessary overhead because of how values are handled. I want to explore ways to streamline this process and cut down on that extra work. I found out that Poul Broennum did some interesting research in this area before, which gives me a solid starting point. 

I’m excited about the potential of this project, but I know there will be challenges along the way, especially in figuring out how to implement the type system effectively. Overall, I think this could really enhance how we use functions in spreadsheets, making them more efficient and reliable."
Peter,Sestoft,"I’m really excited about diving into a project that focuses on using graphics cards, or GPGPUs, for computations. These things are like supercharged computers and can handle certain tasks way faster than regular CPUs—like 10 to 300 times quicker! However, programming them isn’t as straightforward as it sounds; it involves using specific tools and libraries like Nvidia CUDA or OpenCL, which can be a bit tricky to get the hang of.

For my project, I want to work on some numerical computations, especially in linear algebra, since it’s such a fundamental area in math and computer science. I plan to not only implement these computations but also measure how well they perform. I’m really interested in figuring out how to tweak them for the best possible speed. I know there will be challenges along the way, especially with optimizing performance, but I’m eager to learn and see what I can achieve!"
Peter,Sestoft,"I’m really interested in diving into the C5 generic collection library for C#/.Net 2.0. It’s packed with cool features like sublist views, update events, and persistent trees, which is awesome. But I’ve noticed that all these extra functionalities can lead to higher memory usage, even if you’re not using them. 

For my project, I want to explore how we can automatically generate or pick only the parts of the library that are actually needed for specific applications. This way, we can cut down on memory use and speed things up. I think it’ll be a fun challenge to figure out how to streamline the library while still keeping its powerful capabilities. Plus, I’m curious to see how this could improve performance in real-world scenarios."
Peter,Sestoft,"I’m really interested in exploring how run-time code generation works in Java or C#. Both the Java platform from Oracle and the .NET framework from Microsoft have some cool features that let programs create and run bytecode on the fly. This capability could lead to some exciting new ways to handle things like serialization and deserialization, communication protocols, encryption, and even image processing tasks. 

I think it would be fascinating to dive into how these technologies can be applied in real-world scenarios. For instance, figuring out how to make data transfer more efficient or enhancing security through dynamic code generation could be really impactful. Plus, I’m curious about the challenges that come with implementing these ideas, like performance issues or debugging complexities. Overall, I see a lot of potential in this area, and I’m eager to see where it leads!"
Rob,van der Goot,"I'm really interested in exploring how to train NLP models using data from different sources. It seems like knowing where the text comes from could really help the model perform better, especially since these datasets often have different characteristics. 

One idea I have is to use dataset embeddings, which could involve tagging the data with something like [SOCIAL] or [NEWS] at the beginning. This way, the model would have a clue about the context right from the start. Another approach could be to have the model guess the source of the input as an extra task, which might help it learn more effectively.

While I've seen some promising results with these methods in separate experiments, I noticed that there hasn't been a thorough comparison of them across different setups. I think diving into this could uncover some valuable insights, and I'm excited about the potential challenges and discoveries along the way."
Rob,van der Goot,"I'm really interested in exploring lexical normalization, which is all about changing non-standard language into standard language at the word level. Most of the research so far has been focused on Twitter, but I think there’s a lot more out there that needs attention. For my project, I want to dive into two main questions: first, how much does the performance of these models drop when we look at different types of non-standard language? And second, what can we do to create more reliable models for normalization?

I’ve found that a lot of existing datasets, like those from MultiLexNorm, are super helpful for this kind of work. I’ll need to gather and label some data, but thankfully, the annotation process for this project shouldn’t take too long. I’m excited about the challenges ahead and can’t wait to see what I discover!"
Rob,van der Goot,"I’m really interested in exploring how we can use agents to adapt children’s cartoons for different languages while keeping cultural nuances in mind. Cartoons are such a fun way for kids to pick up a new language, but the problem is that they usually only come in a handful of languages. Plus, translating them isn’t just about swapping words; it’s a complex process that involves understanding the cultural context and making sure the dialogue sounds natural, since a lot of it is conversational. 

To tackle this, I think we could use different NLP models, or agents, for each part of the adaptation process, like translating the dialogue and generating speech. However, one big hurdle I see is figuring out how to evaluate the quality of these adaptations. I think we’ll probably need some human feedback to really get it right. Overall, I’m excited about the potential of this project and the challenges it presents!"
Rob,van der Goot,"Lately, I've been really intrigued by how sequence-to-sequence (seq2seq) models are gaining traction. I came across a paper that suggests these models can actually compete with traditional sequence classification models by turning tasks into sequence generation challenges. But the way they compared them wasn’t exactly straightforward, which got me thinking: it would be cool to dive deeper and directly compare an auto-encoder language model with a generative one. I’m curious about how well seq2seq models can handle tasks like sequence tagging and structure parsing, and I think there’s a lot of potential there that hasn’t been fully explored yet. This project could help shed some light on their true capabilities and maybe even highlight some challenges along the way."
Rob,van der Goot,"I've been thinking a lot about how NLP models struggle with grasping cultural aspects, and I really want to dive into this for my project. It seems like there’s a lot of room for improvement in this area. One idea I have is to try predicting cultural dimensions, like those from Hofstede, just by analyzing texts. I think it would be fascinating to see if we can extract cultural insights from the way people write. 

Another angle I’m considering is how we could integrate these cultural insights into the training and prediction phases of NLP models. It feels like there’s a big gap here, and I’m curious about how we can bridge it. I know there are challenges, like figuring out the right texts to analyze and ensuring the models actually learn from the cultural data. But I’m excited about the potential impact this could have on making NLP tools more culturally aware. Overall, I think this project could really contribute to a better understanding of how language and culture interact."
Rob,van der Goot,"I'm really interested in exploring how temperature affects the way language models generate text. When we use these models, we want the output to sound more like something a person would say, right? So, temperature is this cool tool that helps tweak how random or predictable the text is. It plays a big role in shaping the ""vibe"" of what the model produces.

For my project, I want to dive into how changing the temperature impacts how human-like the generated text feels, based on feedback from actual people. I think it’ll be fascinating to see if there’s a sweet spot where the text feels just right. Plus, I plan to experiment with different decoding methods for generating text using Transformers, which should add another layer to my findings. I’m excited about the challenges this might bring, especially in figuring out the best ways to evaluate the human-ness of the text."
Rob,van der Goot,"I’m really interested in exploring how we decide when to stop training neural models, especially in natural language processing. Usually, people use a development set to figure out the right moment to halt training, but I’ve noticed that this method has its flaws. There’s been some talk about the issues with splitting data into train, dev, and test sets, which makes me think there might be better ways to go about it.

I’ve come across a few different strategies for early stopping and model selection. For instance, some folks suggest just training for a set number of steps, while others recommend keeping an eye on the training loss. But as far as I can tell, no one has really compared these methods head-to-head. I think it would be super valuable to dive into this and see which approach works best. Plus, I’m curious about how these strategies might help pretrained models actually learn what we want them to do, especially when it comes to cross-lingual tasks. This project could really shed some light on a topic that feels a bit murky right now!"
Rob,van der Goot,"I've been thinking a lot about how different social backgrounds affect the way we use language, especially in online spaces. There’s been some cool research showing that knowing where a piece of text comes from can really help with natural language processing tasks. But what I find interesting is that we still don’t really know which specific social factors, like age or gender, actually influence how people communicate.

I’ve noticed that recent studies on social media could really shed some light on this. For example, there’s been talk about how women might have a different way of structuring sentences compared to men, and how that could affect things like part-of-speech tagging and parsing. Plus, it seems like there are variations in syntax based on age and gender across different languages, which is pretty fascinating.

I’m excited to dive into this topic because I think understanding these differences could help improve how we analyze and process language in tech. But I also know it’s going to be a challenge to figure out the connections between these sociodemographic factors and language use. I’m looking forward to exploring this further and seeing what I can uncover!"
Rob,van der Goot,"I'm really interested in diving into the whole instruction tuning thing, which is basically when a language model learns how to handle different tasks. Lately, there’s been a ton of datasets made for English, but when it comes to other languages, there’s hardly any manually created stuff out there. Most of the time, folks just translate English instructions or use outputs from bigger, more reliable language models. What I find intriguing is that there hasn’t been a solid comparison of these methods yet. For my project, I want to explore how much data is needed and what the costs look like for each approach. It’ll be interesting to see which method is more effective and if one stands out over the others."
Rob,van der Goot,"I’m really interested in exploring how we can make social media language easier to understand across different languages. Lexical normalization is all about taking the slang and shorthand we see online and turning it into something more standard. Most of the research so far has focused on just one language at a time, which seems pretty limiting. 

In 2021, a project called MultiLexNorm came out, and it’s a game-changer because it includes normalization datasets for 13 different language variations. A bunch of models were tested on this new dataset, but the catch is that they all trained separate models for each language. I think there’s a huge opportunity here to use at least three of those models in a way that could handle multiple languages at once. This could really boost efficiency and performance, especially for languages that don’t have much annotated data available yet.

I’m excited about the potential of this approach and how it could help bridge gaps in understanding across different languages. I want to dive deeper into this topic and see how we can make it work better for everyone."
Rob,van der Goot,"I want to dive into the world of social media data and how we break it down into tokens for natural language processing. While tokenization is pretty straightforward for regular text, social media posts throw a lot of curveballs that make it tricky. My plan is to build a multilingual dataset and a model that can handle this challenge.

To kick things off, I’ll start by gathering the original posts from Multi-LexNorm. Then, I’ll create a solid dataset that compares the original text with the tokenized versions. After that, I’ll check out some existing tokenizers to see how they perform and maybe even develop my own. I’ve come across some interesting work like Universal Word Segmentation and tools like twokenize and nltk.TweetTokenizer, which I think will be helpful.

I’m really excited about this project because social media language is so dynamic and constantly evolving. It’ll be a challenge, but I think it’s super important to get it right, especially as we rely more on these platforms for communication."
Rob,van der Goot,"I’m really interested in diving into language identification, which is a key part of natural language processing. Even though it seems like this area is pretty well covered, I’ve noticed that most tools out there only work with about 100 languages, and many aren’t even available for public use. That’s where my project comes in. I want to explore the LTI LangID Corpus, which has over 1300 languages, and figure out how we can effectively manage such a huge range of languages and different types of input features.

I’ve come across some previous studies that tackle similar issues, like improving identification for a ton of languages and creating fast, compact models for mixed-language texts. However, I think there are still a lot of challenges to address, especially when it comes to real-world applications. I’m excited to see how we can push the boundaries in this field and make language identification more accessible and efficient for everyone."
Rob,van der Goot,"I've been thinking about diving into language identification, especially how it works across different domains. A lot of the models out there seem to focus on just one specific area, which makes me wonder how well they hold up when faced with different types of text. I believe it would be super interesting to create a dataset that covers multiple domains. This way, I could test how robust these existing models really are.

I’ve come across some cool resources that could help with this project. For instance, there’s the Accurate Language Identification of Twitter Messages, which could provide a unique angle since social media language can be pretty informal and varied. Then there's MassiveSumm, a huge dataset for news summarization that includes a ton of different languages. Plus, the LTI LangID Corpus and Fandom Wiki could offer diverse content that might challenge the models in new ways.

I’m excited about the potential challenges this project could bring, like figuring out how to compile and balance the dataset effectively. It’s a bit daunting, but I think it could lead to some valuable insights into how well these models really perform in real-world scenarios. Overall, I’m really looking forward to exploring this topic further!"
Rob,van der Goot,"I’m really interested in diving into dependency parsing, especially when it comes to Danish social media data. Basically, dependency parsing is all about figuring out how words in a sentence relate to each other. I found a great intro to this in Chapter 14 of the Speech and Language Processing book, which got me thinking about how this applies to different languages.

I’ve noticed that while the Universal Dependencies project has tackled a bunch of languages, there are still some that don’t get as much attention, and that’s where things can get tricky. Recently, I came across some non-standard data samples called DaN+, and I’m curious about how well the current parsing methods would work on them.

For my project, I want to annotate a small chunk of Danish social media posts to see how well different parsers can handle it. I think it’ll be interesting to explore various ways to tweak the parser to improve its performance. Some ideas I’m considering include looking at challenges in annotating and parsing data that mixes languages, like code-switching, and comparing different methods for parsing languages that don’t have a lot of resources available. 

I’m also thinking about how to approach parsing low-resource languages, whether it’s through cross-lingual parsing or focusing on annotating the target language. Plus, I want to look into how uncertainty in input affects neural network dependency parsing. Overall, I’m excited about the potential findings and the challenges I might face along the way!"
Rob,van der Goot,"I’m really interested in diving into morphological tagging for my project. Basically, it’s about giving labels to words based on their structure, which can get pretty complex since a single word might have multiple labels. There are a bunch of different methods out there, but it’s not super clear which ones work best in different scenarios.

For this project, I plan to use the Universal Dependencies dataset, which has a ton of languages with morphological tags already annotated. I’m thinking about using the MaChAmp toolkit, but I might also try building a BiLSTM tagger from scratch to see how that goes.

I want to explore at least three main strategies for tagging. One approach is to treat the tags as a single label, similar to how we do with POS tagging, but with more labels involved. Another method could be to predict the tags in a sequence, kind of like how machine translation works. Lastly, I’m considering looking at it as a multilabel prediction problem, where I’d get probabilities for each label and then set a threshold to decide which ones to keep.

I’ve come across some interesting readings, like the SIGMORPHON 2019 Shared Task, which focuses on morphological analysis in different contexts. I’m excited to see what I can learn and hopefully contribute to this area, even though I know there will be challenges along the way!"
Rodrigo,Moreno Garcia,"I'm really excited about the idea of diving into a project that looks at how groups of agents evolve together in a huge game setting. I want to use the Neural MMO environment, which I found through OpenAI, to explore how populations of thousands or even millions of agents, all controlled by neural networks, can develop over time.

One of the main questions I have is about the conditions that lead to cool group dynamics. Like, what makes these agents start to form different species? Are there specific environmental factors that push them to work together or compete against each other? I think it would be fascinating to see how these interactions play out and what influences their behavior.

I know there are a lot of challenges in this area, especially when it comes to managing such large populations and figuring out how to analyze the data. But I’m really curious about how these agents adapt and change based on their surroundings. I believe this project could shed light on some interesting aspects of cooperation and competition in artificial intelligence, and I can’t wait to get started!"
Rune,Møller Jensen,"I'm really excited about diving into a project focused on optimizing how we stow containers on ships. The whole stowage planning thing is super complex, and honestly, trying to find the perfect solution feels a bit overwhelming. So, my aim is to come up with some quick and reliable approximation methods instead. I think there are a bunch of different strategies I could explore, like using specific heuristics, large neighborhood searches, and maybe even some integer programming. I’m also curious about how hierarchical decompositions could fit into this. It’s a challenging area, but I’m eager to tackle it and see what I can come up with!"
